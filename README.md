# ğŸ“ˆ Stock Market Forecasting System (Simplified)

A simple and functional Stock Market Forecasting Platform with Machine Learning models.

## ğŸ¯ Features

- **Local CSV Data**: Works with historical stock data from CSV files
- **ML Models**: ARIMA and Prophet for time series forecasting
- **REST API**: FastAPI service with interactive documentation
- **Easy to Run**: Simple setup and execution

## ğŸš€ Quick Start

### 1. Install Dependencies

```powershell
pip install -r requirements.txt
```

### 2. Run the API

```powershell
python run.py
```

The API will start on `http://localhost:8000`

### 3. Access API Documentation

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

## ğŸ“Š API Endpoints

### List Available Stocks

```http
GET /api/v1/stocks
```

### Get Stock Data

```http
GET /api/v1/data/{symbol}?limit=100
```

### Get Latest Price

```http
GET /api/v1/latest/{symbol}
```

### Generate Predictions

```http
POST /api/v1/predict
Content-Type: application/json

{
  "symbol": "RELIANCE",
  "days": 30,
  "model_type": "auto"
}
```

Model types: `auto`, `arima`, `prophet`

## ğŸ“¦ Dependencies

- pandas, numpy - Data manipulation
- yfinance - Fetch live stock data (optional)
- scikit-learn - Machine learning utilities
- statsmodels - ARIMA model
- prophet - Facebook Prophet model
- fastapi, uvicorn - Web API framework

## ğŸ“ Models

- **ARIMA**: Auto-regressive Integrated Moving Average
- **Prophet**: Facebook's forecasting tool
- **Auto Mode**: Trains both and selects best performer

## ğŸ“„ License

See LICENSE file for details.

## ğŸ¯ Features

- **Real-time Data Ingestion**: Fetch live stock data from YFinance (NSE/BSE markets)
- **Distributed Processing**: PySpark-based ETL pipelines for scalable data processing
- **Advanced ML Models**: ARIMA, LSTM, and Prophet with automatic model selection
- **REST API**: FastAPI service with authentication and rate limiting
- **Interactive Dashboard**: React-based visualization with real-time predictions
- **Workflow Orchestration**: Airflow DAGs for automated data pipelines
- **Cloud Storage**: AWS S3 integration for data persistence
- **Enterprise Features**: Logging, monitoring, retry mechanisms, and security

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  YFinance API   â”‚
â”‚   (NSE/BSE)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Data Ingestion Layer              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ YFinance â”‚      â”‚ CSV Upload   â”‚    â”‚
â”‚  â”‚ Fetcher  â”‚      â”‚   Handler    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         AWS S3 Storage                   â”‚
â”‚  /raw/  /processed/  /predictions/      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      PySpark ETL & Feature Eng.         â”‚
â”‚  â€¢ Data Cleaning                         â”‚
â”‚  â€¢ Technical Indicators (RSI, MACD)      â”‚
â”‚  â€¢ Rolling Windows                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ML Model Training                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ ARIMA  â”‚ â”‚  LSTM  â”‚ â”‚ Prophet â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚       Auto-selection by RMSE            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        FastAPI Service                   â”‚
â”‚  â€¢ Prediction Endpoints                  â”‚
â”‚  â€¢ Authentication                        â”‚
â”‚  â€¢ Rate Limiting                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       React Dashboard                    â”‚
â”‚  â€¢ Stock Charts                          â”‚
â”‚  â€¢ Prediction Visualization              â”‚
â”‚  â€¢ Data Upload Interface                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

       Orchestrated by Apache Airflow
```

## ğŸ“ Project Structure

```
stock-prediction/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/          # Data fetching and S3 operations
â”‚   â”‚   â”œâ”€â”€ yfinance_fetcher.py
â”‚   â”‚   â”œâ”€â”€ csv_handler.py
â”‚   â”‚   â””â”€â”€ s3_utils.py
â”‚   â”œâ”€â”€ processing/         # PySpark ETL pipelines
â”‚   â”‚   â”œâ”€â”€ etl_pipeline.py
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”‚   â””â”€â”€ technical_indicators.py
â”‚   â”œâ”€â”€ models/            # ML model implementations
â”‚   â”‚   â”œâ”€â”€ arima_model.py
â”‚   â”‚   â”œâ”€â”€ lstm_model.py
â”‚   â”‚   â”œâ”€â”€ prophet_model.py
â”‚   â”‚   â””â”€â”€ model_selector.py
â”‚   â”œâ”€â”€ api/               # FastAPI service
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ endpoints.py
â”‚   â”‚   â”œâ”€â”€ auth.py
â”‚   â”‚   â””â”€â”€ middleware.py
â”‚   â”œâ”€â”€ utils/             # Utilities
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â””â”€â”€ exceptions.py
â”‚   â””â”€â”€ config/            # Configuration files
â”‚       â””â”€â”€ settings.py
â”œâ”€â”€ infra/                 # Infrastructure setup
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ airflow/
â”‚   â”‚   â””â”€â”€ dags/
â”‚   â””â”€â”€ spark/
â”‚       â””â”€â”€ spark-defaults.conf
â”œâ”€â”€ dashboard/             # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ public/
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ tests/                 # Unit tests
â”‚   â”œâ”€â”€ test_ingestion.py
â”‚   â”œâ”€â”€ test_processing.py
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ scripts/              # Helper scripts
â”‚   â”œâ”€â”€ run_local.sh
â”‚   â”œâ”€â”€ deploy.sh
â”‚   â””â”€â”€ setup.sh
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci-cd.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .env.template
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- Docker & Docker Compose
- AWS Account (for S3)
- Node.js 18+ (for dashboard)

### Installation

1. **Clone the repository**

```bash
git clone <repository-url>
cd stock-prediction
```

2. **Set up environment variables**

```bash
cp .env.template .env
# Edit .env with your AWS credentials and configuration
```

3. **Install Python dependencies**

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

4. **Start infrastructure with Docker**

```bash
cd infra
docker-compose up -d
```

5. **Run the FastAPI service**

```bash
python -m src.api.main
```

6. **Start the dashboard**

```bash
cd dashboard
npm install
npm run dev
```

### Using Helper Scripts

**Windows (PowerShell):**

```powershell
.\scripts\setup.ps1
.\scripts\run_local.ps1
```

**Linux/Mac:**

```bash
chmod +x scripts/*.sh
./scripts/setup.sh
./scripts/run_local.sh
```

## ğŸ“Š Usage

### Fetch Stock Data

```bash
curl -X GET "http://localhost:8000/api/v1/fetch?symbol=RELIANCE.NS&start_date=2023-01-01&end_date=2024-01-01" \
  -H "Authorization: Bearer YOUR_TOKEN"
```

### Get Predictions

```bash
curl -X GET "http://localhost:8000/api/v1/predict?symbol=TCS.NS&days=30" \
  -H "Authorization: Bearer YOUR_TOKEN"
```

### Upload CSV Dataset

```bash
curl -X POST "http://localhost:8000/api/v1/upload" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -F "file=@stock_data.csv"
```

### View Historical Data

```bash
curl -X GET "http://localhost:8000/api/v1/history?symbol=INFY.NS" \
  -H "Authorization: Bearer YOUR_TOKEN"
```

## ğŸ” Security Features

- **JWT Authentication**: Token-based API authentication
- **Rate Limiting**: 60 requests per minute per client
- **S3 Encryption**: Server-side encryption for all S3 objects
- **IAM Roles**: Least-privilege access policies
- **API Logging**: Comprehensive audit trail
- **Input Validation**: Request validation with Pydantic

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test file
pytest tests/test_api.py -v
```

## ğŸ”„ CI/CD Pipeline

GitHub Actions workflow automatically:

- Runs linting and formatting checks
- Executes unit tests
- Builds Docker images
- Deploys to staging/production

## ğŸ“ˆ Monitoring

- **Prometheus**: Metrics collection at `/metrics`
- **Logs**: Structured JSON logs in `logs/app.log`
- **Airflow UI**: DAG monitoring at `http://localhost:8080`
- **API Health Check**: `GET /health`

## ğŸ¯ Scalability Improvements

### Current Architecture

- Single-node Spark processing
- Local file caching
- API rate limiting

### Recommended Enhancements

1. **Distributed Spark Cluster**

   - Set up EMR or Databricks cluster
   - Configure YARN or Kubernetes executor
   - Enable dynamic resource allocation

2. **Caching Layer**

   - Add Redis for API response caching
   - Cache frequently accessed predictions
   - Implement cache invalidation strategy

3. **Load Balancing**

   - Deploy multiple API instances
   - Use AWS ALB or NGINX
   - Implement health checks

4. **Database Optimization**

   - Use TimescaleDB for time-series data
   - Implement read replicas
   - Add connection pooling

5. **Async Processing**

   - Use Celery for background tasks
   - Implement message queue (SQS/RabbitMQ)
   - Add result backend (Redis)

6. **Model Serving**

   - Deploy models to SageMaker
   - Use TensorFlow Serving for LSTM
   - Implement A/B testing

7. **Monitoring & Alerting**

   - Set up Grafana dashboards
   - Configure PagerDuty alerts
   - Add distributed tracing (Jaeger)

8. **Data Partitioning**
   - Partition S3 data by date/symbol
   - Use Apache Hudi for incremental processing
   - Implement data versioning

## ğŸ› ï¸ Configuration

Key configuration options in `.env`:

| Variable                | Description                                     | Default |
| ----------------------- | ----------------------------------------------- | ------- |
| `MODEL_TYPE`            | ML model selection (auto, arima, lstm, prophet) | `auto`  |
| `PREDICTION_DAYS`       | Number of days to forecast                      | `30`    |
| `RATE_LIMIT_PER_MINUTE` | API rate limit                                  | `60`    |
| `SPARK_DRIVER_MEMORY`   | Spark driver memory                             | `4g`    |
| `LOG_LEVEL`             | Logging level                                   | `INFO`  |

## ğŸ› Troubleshooting

### Common Issues

**Spark Out of Memory**

```bash
# Increase memory in .env
SPARK_DRIVER_MEMORY=8g
SPARK_EXECUTOR_MEMORY=8g
```

**AWS Credentials Error**

```bash
# Verify credentials
aws sts get-caller-identity
```

**Airflow DAG not appearing**

```bash
# Check DAG syntax
python infra/airflow/dags/stock_pipeline.py
```

## ğŸ“ License

MIT License - see LICENSE file for details

## ğŸ‘¥ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Open a Pull Request

## ğŸ“§ Support

For issues and questions:

- Create an issue on GitHub
- Email: support@stockforecasting.com
- Docs: https://docs.stockforecasting.com
