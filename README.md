# üìà Stock Market Prediction with PySpark & LightGBM

A production-ready Stock Market Forecasting Platform featuring distributed data processing with PySpark, advanced ML models, and an interactive React dashboard.

## ‚ú® Key Highlights

- **üî• PySpark Integration**: Distributed data preprocessing for 235K+ records (2-3x faster)
- **ü§ñ LightGBM Model**: Trained on 49 stocks with 232,742 samples (RMSE: 191.59)
- **‚ö° Fast Predictions**: <100ms inference time with 32 technical features
- **üìä Interactive Dashboard**: Next.js React UI with real-time analysis
- **üéØ Multi-Stock Analysis**: Market-wide sentiment and sector performance
- **üîÑ Smart Data Loading**: Automatic CSV validation with YFinance fallback
- **üìà Multiple Models**: LightGBM (primary), ARIMA, Prophet with auto-selection

## üöÄ Quick Start

### Prerequisites

- Python 3.12+
- Node.js 18+ (for frontend)
- Java 8+ (for PySpark)

### 1. Clone Repository

```bash
git clone https://github.com/deekshithgowda85/Stock-Market-Prediction.git
cd Stock-Market-Prediction
```

### 2. Install Python Dependencies

```bash
pip install -r requirements.txt
```

### 3. Train Model (Optional - Pre-trained model included)

```bash
python models/train_multi_stock_lightgbm.py
```

**Training Details**:

- Uses PySpark for distributed preprocessing
- Processes 232,742 samples from 49 stocks
- Takes ~3 seconds to train
- Model saved to `models/multi_stock_lightgbm/`

### 4. Start Backend API

```bash
python run.py
```

API runs on `http://localhost:8000`

**Note**: First request may take 40 seconds as PySpark initializes Spark session

### 5. Start Frontend Dashboard (Optional)

```bash
cd frontend
npm install
npm run dev
```

Dashboard runs on `http://localhost:3000`

### 6. Access API Documentation

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

## üìä API Endpoints

### Data Operations (PySpark-Powered)

**List Available Stocks**

```http
GET /api/v1/stocks
```

Returns 52 NIFTY 50 stock symbols

**Get Stock Data** (üî• PySpark)

```http
GET /api/v1/data/{symbol}?limit=100&force_update=false
```

- Uses PySpark for distributed data loading
- Returns last 100 records by default
- Set `force_update=true` to fetch live data from YFinance

**Stock Analysis** (üî• PySpark)

```http
GET /api/v1/analyze/{symbol}?preprocess=true&auto_update=false
```

- PySpark loads and cleans data
- Returns technical indicators (RSI, MACD, Bollinger Bands)
- Calculates volatility, returns, moving averages

### Prediction Operations

**Generate Predictions** (üî• PySpark + ML)

```http
POST /api/v1/predict
Content-Type: application/json

{
  "symbol": "RELIANCE",
  "days": 30,
  "model_type": "auto"
}
```

- PySpark preprocesses data
- Model types: `auto`, `arima`, `prophet`
- Returns 30-day forecast with confidence intervals

**LightGBM Predictions** (Primary Model)

```http
POST /api/v1/predict-lightgbm
Content-Type: application/json

{
  "symbol": "RELIANCE",
  "days": 30
}
```

- Fast predictions (<100ms)
- Trained on 232K samples
- 32 technical features
- Best for short-term forecasts (7-30 days)

**Multi-Stock Market Analysis**

```http
POST /api/v1/analyze-market
Content-Type: application/json

{
  "symbols": ["RELIANCE", "INFY", "TCS", "HDFCBANK"],
  "days": 30
}
```

- Bulk predictions for multiple stocks
- Market sentiment analysis
- Sector performance comparison
- Top gainers/losers identification

## üéØ Key Features

### Backend

- **üî• PySpark Integration**: Distributed data preprocessing (2-3x faster than pandas)
- **‚ö° LightGBM Model**: Primary prediction engine with 191.59 RMSE
- **üìà Multiple Models**: ARIMA, Prophet with automatic model selection
- **üîÑ Smart Data Loading**: CSV-first with YFinance fallback for fresh data
- **üìä Technical Analysis**: 35 indicators (RSI, MACD, Bollinger Bands, etc.)
- **üéØ Multi-Stock Support**: Train and predict across 49 stocks simultaneously
- **‚è±Ô∏è Fast Inference**: <100ms predictions with pre-trained models

### Frontend

- **üì± Interactive Dashboard**: Real-time stock analysis and visualization
- **üìä Advanced Charts**: Recharts with historical data and predictions
- **üåê Market Overview**: Multi-stock analysis with sentiment indicators
- **üìà Sector Performance**: Compare different market sectors
- **‚ö° Real-time Updates**: Live data fetching from YFinance
- **üé® Modern UI**: Tailwind CSS with responsive design

### DevOps & Infrastructure

- **üîÑ CI/CD Pipeline**: GitHub Actions for automated testing and deployment
- **üê≥ Docker Support**: Containerized deployment (optional)
- **üìù Comprehensive Logging**: Structured logs for debugging and monitoring
- **üîê API Documentation**: Auto-generated Swagger UI and ReDoc
- **‚ö†Ô∏è Error Handling**: Graceful fallbacks and informative error messages

## üì¶ Technology Stack

### Backend

| Category  | Technology  | Version | Purpose                   |
| --------- | ----------- | ------- | ------------------------- |
| Language  | Python      | 3.12    | Core backend              |
| Framework | FastAPI     | Latest  | REST API                  |
| Big Data  | PySpark     | 3.5.0   | Distributed preprocessing |
| ML        | LightGBM    | 4.6.0   | Primary model             |
| ML        | Prophet     | Latest  | Time series forecasting   |
| ML        | Statsmodels | Latest  | ARIMA implementation      |
| Data      | Pandas      | Latest  | Data manipulation         |
| Data      | NumPy       | Latest  | Numerical operations      |
| Server    | Uvicorn     | Latest  | ASGI server               |

### Frontend

| Category  | Technology   | Version | Purpose            |
| --------- | ------------ | ------- | ------------------ |
| Framework | Next.js      | 14      | React framework    |
| Language  | TypeScript   | 5.x     | Type safety        |
| UI        | React        | 18      | Component library  |
| Styling   | Tailwind CSS | 3.x     | Utility-first CSS  |
| Charts    | Recharts     | 2.x     | Data visualization |
| Build     | Turbopack    | Latest  | Fast bundler       |

### Infrastructure

- **Version Control**: Git + GitHub
- **CI/CD**: GitHub Actions
- **Container**: Docker (optional)
- **Package Manager**: pip (Python), npm (Node.js)

## üìÅ Project Structure

```
Stock-prediction/
‚îú‚îÄ‚îÄ dataset/                   # 52 NIFTY 50 stock CSV files (51.84 MB)
‚îú‚îÄ‚îÄ frontend/                  # Next.js React dashboard
‚îÇ   ‚îú‚îÄ‚îÄ app/                  # Next.js 14 App Router
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page.tsx         # Main dashboard
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ multi-stock/     # Market analysis page
‚îÇ   ‚îî‚îÄ‚îÄ components/           # React components
‚îú‚îÄ‚îÄ models/                    # Trained ML models
‚îÇ   ‚îú‚îÄ‚îÄ multi_stock_lightgbm/ # LightGBM model files
‚îÇ   ‚îî‚îÄ‚îÄ train_multi_stock_lightgbm.py # Training script
‚îú‚îÄ‚îÄ src/                       # Python backend
‚îÇ   ‚îú‚îÄ‚îÄ api/                  # FastAPI endpoints
‚îÇ   ‚îú‚îÄ‚îÄ config/               # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ ingestion/            # Data loading (CSV, YFinance)
‚îÇ   ‚îú‚îÄ‚îÄ models/               # ML model implementations
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/        # PySpark data preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ processing/           # Feature engineering
‚îÇ   ‚îî‚îÄ‚îÄ utils/                # Helper functions
‚îú‚îÄ‚îÄ .github/workflows/        # CI/CD pipelines
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ run.py                    # Backend launcher
‚îî‚îÄ‚îÄ PROJECT_STRUCTURE.md      # Detailed folder documentation
```

See [PROJECT_STRUCTURE.md](PROJECT_STRUCTURE.md) for detailed folder explanations.

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Frontend (Next.js)                        ‚îÇ
‚îÇ  ‚Ä¢ Dashboard Page (Single Stock Analysis)                    ‚îÇ
‚îÇ  ‚Ä¢ Multi-Stock Page (Market Overview)                        ‚îÇ
‚îÇ  ‚Ä¢ React Components (Charts, Analysis, Predictions)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ HTTP/JSON API
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                FastAPI Backend (Python 3.12)                  ‚îÇ
‚îÇ  ‚Ä¢ /data/{symbol} - PySpark data loading                     ‚îÇ
‚îÇ  ‚Ä¢ /analyze/{symbol} - PySpark analysis                      ‚îÇ
‚îÇ  ‚Ä¢ /predict - PySpark + ML predictions                       ‚îÇ
‚îÇ  ‚Ä¢ /predict-lightgbm - LightGBM predictions                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ                ‚îÇ
             ‚ñº                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  SparkPreprocessor‚îÇ  ‚îÇ   ML Models         ‚îÇ
‚îÇ  (PySpark 3.5)   ‚îÇ  ‚îÇ                     ‚îÇ
‚îÇ  ‚Ä¢ Load CSV      ‚îÇ  ‚îÇ  ‚Ä¢ LightGBM ‚≠ê      ‚îÇ
‚îÇ  ‚Ä¢ Clean Data    ‚îÇ  ‚îÇ  ‚Ä¢ ARIMA           ‚îÇ
‚îÇ  ‚Ä¢ Deduplicate   ‚îÇ  ‚îÇ  ‚Ä¢ Prophet         ‚îÇ
‚îÇ  ‚Ä¢ Validate      ‚îÇ  ‚îÇ  ‚Ä¢ Model Selector  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ
         ‚ñº toPandas()           ‚ñº predict()
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Feature Engineering (Pandas)         ‚îÇ
‚îÇ  ‚Ä¢ 32 Technical Indicators               ‚îÇ
‚îÇ  ‚Ä¢ MAs, RSI, MACD, Bollinger Bands       ‚îÇ
‚îÇ  ‚Ä¢ Volatility, Returns, Lag Features     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Dataset (CSV Files)              ‚îÇ
‚îÇ  ‚Ä¢ 52 NIFTY 50 Stocks                    ‚îÇ
‚îÇ  ‚Ä¢ 235,192 Total Records                 ‚îÇ
‚îÇ  ‚Ä¢ 51.84 MB Data (2000-2021)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Flow

1.  **Ingestion**: PySpark loads CSV ‚Üí Distributed cleaning ‚Üí Pandas DataFrame
2.  **Processing**: Feature engineering creates 32 technical indicators
3.  **Training**: LightGBM trains on 232K samples (one-time, 3 seconds)
4.  **Inference**: Load model ‚Üí Generate predictions ‚Üí Return to API
5.  **Display**: Frontend fetches predictions ‚Üí Renders charts and analysis
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ YFinance API ‚îÇ
    ‚îÇ (NSE/BSE) ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Data Ingestion Layer ‚îÇ
    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
    ‚îÇ ‚îÇ YFinance ‚îÇ ‚îÇ CSV Upload ‚îÇ ‚îÇ
    ‚îÇ ‚îÇ Fetcher ‚îÇ ‚îÇ Handler ‚îÇ ‚îÇ
    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ AWS S3 Storage ‚îÇ
    ‚îÇ /raw/ /processed/ /predictions/ ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ PySpark ETL & Feature Eng. ‚îÇ
    ‚îÇ ‚Ä¢ Data Cleaning ‚îÇ
    ‚îÇ ‚Ä¢ Technical Indicators (RSI, MACD) ‚îÇ
    ‚îÇ ‚Ä¢ Rolling Windows ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ ML Model Training ‚îÇ
    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
    ‚îÇ ‚îÇ ARIMA ‚îÇ ‚îÇ LSTM ‚îÇ ‚îÇ Prophet ‚îÇ ‚îÇ
    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
    ‚îÇ Auto-selection by RMSE ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ FastAPI Service ‚îÇ
    ‚îÇ ‚Ä¢ Prediction Endpoints ‚îÇ
    ‚îÇ ‚Ä¢ Authentication ‚îÇ
    ‚îÇ ‚Ä¢ Rate Limiting ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ React Dashboard ‚îÇ
    ‚îÇ ‚Ä¢ Stock Charts ‚îÇ
    ‚îÇ ‚Ä¢ Prediction Visualization ‚îÇ
    ‚îÇ ‚Ä¢ Data Upload Interface ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

           Orchestrated by Apache Airflow

```

## üìÅ Project Structure

```

stock-prediction/
‚îú‚îÄ‚îÄ src/
‚îÇ ‚îú‚îÄ‚îÄ ingestion/ # Data fetching and S3 operations
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ yfinance_fetcher.py
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ csv_handler.py
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ s3_utils.py
‚îÇ ‚îú‚îÄ‚îÄ processing/ # PySpark ETL pipelines
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ etl_pipeline.py
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ feature_engineering.py
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ technical_indicators.py
‚îÇ ‚îú‚îÄ‚îÄ models/ # ML model implementations
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ arima_model.py
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ lstm_model.py
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ prophet_model.py
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ model_selector.py
‚îÇ ‚îú‚îÄ‚îÄ api/ # FastAPI service
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ main.py
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ endpoints.py
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ auth.py
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ middleware.py
‚îÇ ‚îú‚îÄ‚îÄ utils/ # Utilities
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ logger.py
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ config.py
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ exceptions.py
‚îÇ ‚îî‚îÄ‚îÄ config/ # Configuration files
‚îÇ ‚îî‚îÄ‚îÄ settings.py
‚îú‚îÄ‚îÄ infra/ # Infrastructure setup
‚îÇ ‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ ‚îú‚îÄ‚îÄ airflow/
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ dags/
‚îÇ ‚îî‚îÄ‚îÄ spark/
‚îÇ ‚îî‚îÄ‚îÄ spark-defaults.conf
‚îú‚îÄ‚îÄ dashboard/ # React frontend
‚îÇ ‚îú‚îÄ‚îÄ src/
‚îÇ ‚îú‚îÄ‚îÄ public/
‚îÇ ‚îî‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ tests/ # Unit tests
‚îÇ ‚îú‚îÄ‚îÄ test_ingestion.py
‚îÇ ‚îú‚îÄ‚îÄ test_processing.py
‚îÇ ‚îî‚îÄ‚îÄ test_api.py
‚îú‚îÄ‚îÄ scripts/ # Helper scripts
‚îÇ ‚îú‚îÄ‚îÄ run_local.sh
‚îÇ ‚îú‚îÄ‚îÄ deploy.sh
‚îÇ ‚îî‚îÄ‚îÄ setup.sh
‚îú‚îÄ‚îÄ .github/
‚îÇ ‚îî‚îÄ‚îÄ workflows/
‚îÇ ‚îî‚îÄ‚îÄ ci-cd.yml
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ .env.template
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md

````

## üöÄ Quick Start

### Prerequisites

- Python 3.10+
- Docker & Docker Compose
- AWS Account (for S3)
- Node.js 18+ (for dashboard)

### Installation

1. **Clone the repository**

```bash
git clone <repository-url>
cd stock-prediction
````

2. **Set up environment variables**

```bash
cp .env.template .env
# Edit .env with your AWS credentials and configuration
```

3. **Install Python dependencies**

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

4. **Start infrastructure with Docker**

```bash
cd infra
docker-compose up -d
```

5. **Run the FastAPI service**

```bash
python -m src.api.main
```

6. **Start the dashboard**

```bash
cd dashboard
npm install
npm run dev
```

### Using Helper Scripts

**Windows (PowerShell):**

```powershell
.\scripts\setup.ps1
.\scripts\run_local.ps1
```

**Linux/Mac:**

```bash
chmod +x scripts/*.sh
./scripts/setup.sh
./scripts/run_local.sh
```

## üìä Usage

### Fetch Stock Data

```bash
curl -X GET "http://localhost:8000/api/v1/fetch?symbol=RELIANCE.NS&start_date=2023-01-01&end_date=2024-01-01" \
  -H "Authorization: Bearer YOUR_TOKEN"
```

### Get Predictions

```bash
curl -X GET "http://localhost:8000/api/v1/predict?symbol=TCS.NS&days=30" \
  -H "Authorization: Bearer YOUR_TOKEN"
```

### Upload CSV Dataset

```bash
curl -X POST "http://localhost:8000/api/v1/upload" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -F "file=@stock_data.csv"
```

### View Historical Data

```bash
curl -X GET "http://localhost:8000/api/v1/history?symbol=INFY.NS" \
  -H "Authorization: Bearer YOUR_TOKEN"
```

## üîê Security Features

- **JWT Authentication**: Token-based API authentication
- **Rate Limiting**: 60 requests per minute per client
- **S3 Encryption**: Server-side encryption for all S3 objects
- **IAM Roles**: Least-privilege access policies
- **API Logging**: Comprehensive audit trail
- **Input Validation**: Request validation with Pydantic

## üß™ Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test file
pytest tests/test_api.py -v
```

## üîÑ CI/CD Pipeline

GitHub Actions workflow automatically:

- Runs linting and formatting checks
- Executes unit tests
- Builds Docker images
- Deploys to staging/production

## üìà Monitoring

- **Prometheus**: Metrics collection at `/metrics`
- **Logs**: Structured JSON logs in `logs/app.log`
- **Airflow UI**: DAG monitoring at `http://localhost:8080`
- **API Health Check**: `GET /health`

## üéØ Scalability Improvements

### Current Architecture

- Single-node Spark processing
- Local file caching
- API rate limiting

### Recommended Enhancements

1. **Distributed Spark Cluster**

   - Set up EMR or Databricks cluster
   - Configure YARN or Kubernetes executor
   - Enable dynamic resource allocation

2. **Caching Layer**

   - Add Redis for API response caching
   - Cache frequently accessed predictions
   - Implement cache invalidation strategy

3. **Load Balancing**

   - Deploy multiple API instances
   - Use AWS ALB or NGINX
   - Implement health checks

4. **Database Optimization**

   - Use TimescaleDB for time-series data
   - Implement read replicas
   - Add connection pooling

5. **Async Processing**

   - Use Celery for background tasks
   - Implement message queue (SQS/RabbitMQ)
   - Add result backend (Redis)

6. **Model Serving**

   - Deploy models to SageMaker
   - Use TensorFlow Serving for LSTM
   - Implement A/B testing

7. **Monitoring & Alerting**

   - Set up Grafana dashboards
   - Configure PagerDuty alerts
   - Add distributed tracing (Jaeger)

8. **Data Partitioning**
   - Partition S3 data by date/symbol
   - Use Apache Hudi for incremental processing
   - Implement data versioning

## üõ†Ô∏è Configuration

Key configuration options in `.env`:

| Variable                | Description                                     | Default |
| ----------------------- | ----------------------------------------------- | ------- |
| `MODEL_TYPE`            | ML model selection (auto, arima, lstm, prophet) | `auto`  |
| `PREDICTION_DAYS`       | Number of days to forecast                      | `30`    |
| `RATE_LIMIT_PER_MINUTE` | API rate limit                                  | `60`    |
| `SPARK_DRIVER_MEMORY`   | Spark driver memory                             | `4g`    |
| `LOG_LEVEL`             | Logging level                                   | `INFO`  |

## üêõ Troubleshooting

### Common Issues

**Spark Out of Memory**

```bash
# Increase memory in .env
SPARK_DRIVER_MEMORY=8g
SPARK_EXECUTOR_MEMORY=8g
```

**AWS Credentials Error**

```bash
# Verify credentials
aws sts get-caller-identity
```

**Airflow DAG not appearing**

```bash
# Check DAG syntax
python infra/airflow/dags/stock_pipeline.py
```

## üìù License

MIT License - see LICENSE file for details

## üë• Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Open a Pull Request

## üìß Support

For issues and questions:

- Create an issue on GitHub
- Email: support@stockforecasting.com
- Docs: https://docs.stockforecasting.com
